Ch. 2 ML/DL 입문 - 분류분석

TensorFlow를 사용하여 두 가지 대표적인 분류 모델인 로지스틱 회귀(이진 분류)와 소프트맥스 회귀(다중 분류)를 각각 저수준(Low-level)과 고수준(High-level) 방식으로 구현.

1. 로지스틱 회귀 (Logistic Regression) - 이진 분류

입력 데이터를 0 또는 1, 두 가지 클래스로 분류하는 모델.

1.1. Low-level 구현 (수동 구현)

시그모이드 함수와 비용 함수를 직접 수식으로 구현하여 원리를 학습한다.

- 데이터: x(입력 2개), y(출력 0 또는 1).
- 가설 (Hypothesis): 시그모이드(Sigmoid) 함수를 직접 구현한다.
    - 수식 코드: tf.div(1., 1. + tf.exp(tf.matmul(x_train, W) + b))
    - 결과값은 0과 1 사이의 확률로 나온다.
- 비용 함수 (Cost Function): 이진 크로스 엔트로피(Binary Cross Entropy) 수식을 직접 구현함.
    - 코드: -tf.reduce_mean(y_train * tf.log(hypothesis) + (1 - y_train) * tf.log(1 - hypothesis))
- 학습: optimizer.minimize()를 사용하여 비용을 최소화한다.

1.2. High-level 구현 (Keras API)

Keras의 활성화 함수와 손실 함수 설정을 통해 간결하게 구현한다.

- 모델 정의: model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    - 출력 뉴런 1개, 활성화 함수로 sigmoid를 지정한다.
- 컴파일: model.compile(loss='binary_crossentropy', optimizer='sgd')
    - 손실 함수로 binary_crossentropy를 지정하는 것이 핵심.

---

2. 소프트맥스 회귀 (Softmax Regression) - 다중 분류

세 개 이상의 클래스로 데이터를 분류하는 모델.

2.1. Low-level 구현 (수동 구현)

각 클래스에 속할 확률을 구하는 소프트맥스 함수와 비용 함수를 구현한다.

- 데이터: x(입력 4개), y(출력 3개, One-hot Encoding된 형태 예: [0, 0, 1]).
- 가중치(W): 입력이 4개이고 출력이 3개(클래스 수)이므로, W의 차원은 [4, 3]이다.
- 가설 (Hypothesis): 텐서플로우 내장 함수인 tf.nn.softmax()를 사용한다.
    - 코드: tf.nn.softmax(tf.matmul(x_train, W) + b)
    - 출력된 3개 값의 합은 항상 1이 된다.
- 비용 함수 (Cost Function): 크로스 엔트로피(Cross Entropy) 수식을 직접 구현한다.
    - 코드: tf.reduce_mean(-tf.reduce_sum(y_train * tf.log(hypothesis), axis=1))

2.2. High-level 구현 (Keras API)

다중 분류를 위한 설정으로 모델을 구축.

- 모델 정의: model.add(tf.keras.layers.Dense(3, activation='softmax'))
    - 출력 뉴런 3개(클래스 개수), 활성화 함수로 softmax를 지정한다.
- 컴파일: model.compile(loss='categorical_crossentropy', optimizer='sgd')
    - 원-핫 인코딩된 레이블을 다루므로 손실 함수로 categorical_crossentropy를 사용한다.

---

※ 핵심 요약

분류 문제의 종류에 따라 달라지는 활성화 함수와 손실 함수의 짝(Pair)을 맞추기.